{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Best Picture Winners & Nominees\n",
    "*An Analysis by Sean Osier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# For display\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickling functions\n",
    "def pickle_it(data, filename, python_version=3):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    data = the data you want to pickle (save)\n",
    "    filename = file name where you want to save the data\n",
    "    python_version = the python version where you will be opening the pickle file\n",
    "    \n",
    "    Out:\n",
    "    Saves a pickle file with your data to to the filename you specify\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as picklefile:\n",
    "        pickle.dump(data, picklefile, protocol=python_version)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    filename = name of the pickle file you want to open (e.g \"my_pickle.pkl\")\n",
    "    \n",
    "    Out:\n",
    "    Opens and returns the content of the picklefile to a variable of your choice\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as picklefile: \n",
    "        return pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_HTML(url):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    url = address of the website whose contents you want to scrape\n",
    "    \n",
    "    Out:\n",
    "    html = the raw HTML of the website for scraping\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    assert (response.status_code >= 200) and (response.status_code < 300)\n",
    "    html = response.text\n",
    "    html = BeautifulSoup(html, \"lxml\")\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape IMDB Movie Index (List) Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_movies_in_year(html):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    html = HTML of an IMDB movie index list page\n",
    "    \n",
    "    Out:\n",
    "    num_movies = the total number of movies in the IMDB movie index list across all pages\n",
    "    \"\"\"\n",
    "    wordy_num_movies = html.find(id=\"left\").text\n",
    "    num_movies = int(wordy_num_movies.split()[2].replace(\",\",\"\"))\n",
    "    return num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_year_page(html):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    html = HTML of an IMDB movie year index list page\n",
    "    \n",
    "    \n",
    "    Out:\n",
    "    movie_data = a list of lists of movie data, each sub-list being a list of the following movie datapoints: title, \n",
    "                 year, link, user_rating_long, user_rating_short, outline, director, starring, genre, pg_rating, \n",
    "                 runtime\n",
    "    \"\"\"\n",
    "    # Extract relevant part of the page and pull out the td's for individual movies\n",
    "    results_table = html.find(class_=\"results\")\n",
    "    even_trs = results_table.find_all(class_=\"even detailed\")\n",
    "    odd_trs = results_table.find_all(class_=\"odd detailed\")\n",
    "    trs = even_trs + odd_trs\n",
    "    tds = []\n",
    "    for tr in trs:\n",
    "        tds += tr.find_all(class_=\"title\")\n",
    "\n",
    "    movie_data = []\n",
    "    for td in tds:\n",
    "        # Movie title\n",
    "        title = td.find(\"a\").text\n",
    "        \n",
    "        # Movie year\n",
    "        year = td.find(class_=\"year_type\").text\n",
    "        \n",
    "        # Link to individual movie page\n",
    "        link = td.find(\"a\")[\"href\"]\n",
    "        \n",
    "        # User (IMDB) rating\n",
    "        try:\n",
    "            user_rating_long = td.find(class_=\"rating rating-list\")[\"title\"]\n",
    "        except:\n",
    "            user_rating_long = \"\"\n",
    "        try:\n",
    "            user_rating_short = td.find(class_=\"value\").text\n",
    "        except:\n",
    "            user_rating_short = \"\"\n",
    "        \n",
    "        # Movie outline / summary\n",
    "        try:\n",
    "            outline = td.find(class_=\"outline\").text\n",
    "        except:\n",
    "            outline = \"\"\n",
    "\n",
    "        # Director / Stars\n",
    "        try:\n",
    "            credits = td.find(class_=\"credit\").text\n",
    "            credits = credits[1:-1].strip().split(\"\\n\")\n",
    "        except:\n",
    "            credits = \"\"    \n",
    "        try:\n",
    "            director = credits[0][5:]\n",
    "        except:\n",
    "            director = \"\"\n",
    "        try:\n",
    "            starring = credits[1].strip()[6:]\n",
    "        except:\n",
    "            starring = \"\"\n",
    "\n",
    "        # Genre\n",
    "        try:\n",
    "            genre = td.find(class_=\"genre\").text\n",
    "        except:\n",
    "            genre = \"\"\n",
    "        \n",
    "        # Parental Guidance rating\n",
    "        try:\n",
    "            pg_rating = td.find(class_=\"certificate\").find(\"span\")[\"title\"]\n",
    "        except:\n",
    "            pg_rating = \"\"\n",
    "        \n",
    "        # Movie runtime\n",
    "        try:\n",
    "            runtime = td.find(class_=\"runtime\").text\n",
    "        except:\n",
    "            runtime = \"\"\n",
    "\n",
    "        movie_data.append([title, year, link, user_rating_long, user_rating_short,\n",
    "                           outline, director, starring, genre, pg_rating, runtime])\n",
    "\n",
    "    return movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_IMBD_movie_list(start_year, end_year):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    start_year = year to start scraping IMDB for\n",
    "    end_year = last year to scrape from IMDB\n",
    "    \n",
    "    Out:\n",
    "    movie_data = a list of lists of movie data, each sub-list being a list of the following movie datapoints: title, \n",
    "                 year, link, user_rating_long, user_rating_short, outline, director, starring, genre, pg_rating, \n",
    "                 runtime\n",
    "    \"\"\"\n",
    "    years = range(start_year, end_year + 1)\n",
    "    movie_data = []\n",
    "\n",
    "    # For each year, go through all the the individual year movie index pages and scrape all the movies\n",
    "    for year in years:\n",
    "        current_n = 1\n",
    "        url = \"http://www.imdb.com/search/title?sort=moviemeter,asc&start=%s&title_type=feature&year=%s,%s\" \\\n",
    "            % (str(current_n), str(year), str(year))\n",
    "        html = get_HTML(url)\n",
    "        num_movies = get_num_movies_in_year(html)\n",
    "        movie_data += scrape_year_page(html)\n",
    "        current_n += 50\n",
    "\n",
    "        while current_n <= num_movies:\n",
    "            url = \"http://www.imdb.com/search/title?sort=moviemeter,asc&start=%s&title_type=feature&year=%s,%s\" \\\n",
    "                % (str(current_n), str(year), str(year))\n",
    "            html = get_HTML(url)\n",
    "            movie_data += scrape_year_page(html)\n",
    "            current_n += 50\n",
    "            \n",
    "    return movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Uncomment this if you want to scrape the data\"\"\"\n",
    "# movie_data = scrape_IMBD_movie_list(1990, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check and make sure you have the expected number of movies in your results\n",
    "len(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Uncomment this when you want to save the data you scraped\"\"\"\n",
    "# pickle_it(movie_data, \"movie_data.pkl\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Wikipedia for Best Picture Winner and Nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_wiki_best_pic_page(html):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    html = html for the wikipedia Best Picture page\n",
    "    \n",
    "    Out:\n",
    "    nominee_data = list of movies categorized \"W\" for Best Picture win and \"N\" for nominated\n",
    "    \"\"\"\n",
    "    nominee_tables = html.find_all(class_=\"wikitable\")\n",
    "    nominee_data = []\n",
    "    \n",
    "    for table in nominee_tables:\n",
    "        year = table.find(\"caption\").find(\"big\").find(\"a\").text\n",
    "        trs = table.find_all(\"tr\")    \n",
    "        trs = trs[1:]\n",
    "        movie_data = []\n",
    "        \n",
    "        for i, tr in enumerate(trs):\n",
    "            title = tr.find(\"td\").find(\"a\").text\n",
    "            if i == 0:\n",
    "                status = \"W\"\n",
    "            else:\n",
    "                status = \"N\"\n",
    "            movie_data.append([title, year, status])\n",
    "            \n",
    "        nominee_data += movie_data\n",
    "    \n",
    "    return nominee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Uncomment these if you want to scrape the data\"\"\"\n",
    "# wiki_html = get_HTML(\"https://en.wikipedia.org/wiki/Academy_Award_for_Best_Picture\")\n",
    "# nominees_and_winners_raw = scrape_wiki_best_pic_page(wiki_html)\n",
    "\n",
    "# Check to make sure the appopriate number of movies were scraped\n",
    "len(nominees_and_winners_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Uncomment this when you want to save the data you scraped\"\"\"\n",
    "# pickle_it(nominees_and_winners_raw, \"nominees_and_winners.pkl\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Individual IMDB Movie Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_individual_movie_page(html):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    html = HTML for an individual IMDB movie page\n",
    "    \n",
    "    Out:\n",
    "    List of movie data containing data points for: release_date, critic_rating, critic_rating_n, writer, country, \n",
    "            language, budget, opening_weekend_gross, production_company, sound_mix, color, aspect_ratio\n",
    "    \"\"\"\n",
    "    # Release Data\n",
    "    try:\n",
    "        release_date = html.find(class_=\"infobar\").find(class_=\"nobr\").text.strip().split(\"\\n\")[0]\n",
    "    except:\n",
    "        release_date = \"\"\n",
    "    \n",
    "    # Critic Rating (Metascore) and n-size\n",
    "    try:\n",
    "        rating_details = html.find(class_=\"star-box-details\").text\n",
    "    except:\n",
    "        rating_details = \"\"\n",
    "    try:\n",
    "        critic_rating = re.search(r\"Metascore: .*/100\", rating_details).group().split()[-1].split(\"/\")[0]\n",
    "        critic_rating_n = re.search(r\"\\|.*\\n.*\\n.*from\\n.*Metacritic.com\", rating_details).group().split(\"\\n\")[1].strip()\n",
    "    except:\n",
    "        critic_rating = \"\"\n",
    "        critic_rating_n = \"\"\n",
    "    \n",
    "    # Writer\n",
    "    try:\n",
    "        writer = [a.text for a in html.find(itemprop=\"creator\").find_all(\"a\")]\n",
    "        if \" credit\" in writer[-1]:\n",
    "            writer = writer[:-1]\n",
    "    except:\n",
    "        writer = \"\"\n",
    "\n",
    "    try:\n",
    "        details_text = html.find(id=\"titleDetails\").text\n",
    "        details_text = details_text.replace(\"\\n\\n\", \"|#|\")\n",
    "        details_text = details_text.replace(\"See full technical specs\", \"|#|\")\n",
    "        details_text = details_text.replace(\"\\n\", \" \")\n",
    "        details_text = details_text.replace(\"\\t\", \" \")\n",
    "        details_text = details_text.replace(\"  \", \" \")\n",
    "    except:\n",
    "        details_text = \"\"\n",
    "    \n",
    "    # Country\n",
    "    if re.search(r\"Country:\", details_text):\n",
    "        country = re.search(r\"Country\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        country = country[8:-3].strip()\n",
    "        country = country.split(\" | \")\n",
    "    else:\n",
    "        country = \"\"\n",
    "    \n",
    "    # Language\n",
    "    if re.search(r\"Language:\", details_text):\n",
    "        language = re.search(r\"Language\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        language = language[9:-3].strip()\n",
    "        language = language.split(\" | \")\n",
    "    else:\n",
    "        language = \"\"\n",
    "    \n",
    "    # Budget\n",
    "    if re.search(r\"Budget:\", details_text):\n",
    "        budget = re.search(r\"Budget\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        budget = budget[7:-3].strip()\n",
    "    else:\n",
    "        budget = \"\"\n",
    "    \n",
    "    # Opening Weekend Gross\n",
    "    if re.search(r\"Opening Weekend:\", details_text):\n",
    "        opening_weekend_gross = re.search(r\"Opening Weekend\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        opening_weekend_gross = opening_weekend_gross[16:-3].strip()\n",
    "    else:\n",
    "        opening_weekend_gross = \"\"\n",
    "    \n",
    "    # Production Company\n",
    "    if re.search(r\"Production Co:\", details_text):\n",
    "        production_company = re.search(r\"Production Co\\:\\|\\#\\|.*?\\|\\#\\|\", details_text).group()\n",
    "        production_company = production_company[17:-3].strip()\n",
    "        production_company = production_company.split(\", \")\n",
    "        if \"See more\" in production_company[-1]:\n",
    "            production_company[-1] = production_company[-1].replace(\"See more\", \"\")\n",
    "            production_company[-1] = production_company[-1].replace(\"\\xa0\", \"\")\n",
    "            production_company[-1] = production_company[-1].replace(\"Â»\", \"\")\n",
    "        production_company = [company.strip() for company in production_company]\n",
    "    else:\n",
    "        production_company = \"\"\n",
    "    \n",
    "    # Sound Mix\n",
    "    if re.search(r\"Sound Mix:\", details_text):\n",
    "        sound_mix = re.search(r\"Sound Mix\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        sound_mix = sound_mix[10:-3].strip()\n",
    "        sound_mix = sound_mix.split(\" | \")\n",
    "    else:\n",
    "        sound_mix = \"\"\n",
    "    \n",
    "    # Color\n",
    "    if re.search(r\"Color:\", details_text):\n",
    "        color = re.search(r\"Color\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        color = color[6:-3].strip()\n",
    "        color = color.split(\" | \")\n",
    "    else:\n",
    "        color = \"\"\n",
    "    \n",
    "    # Aspect Ratio\n",
    "    if re.search(r\"Aspect Ratio:\", details_text):\n",
    "        pass\n",
    "        aspect_ratio = re.search(r\"Aspect Ratio\\:.*?\\|\\#\\|\", details_text).group()\n",
    "        aspect_ratio = aspect_ratio[13:-3].strip()\n",
    "    else:\n",
    "        aspect_ratio = \"\"\n",
    "    \n",
    "    return [release_date, critic_rating, critic_rating_n, writer, country, language, \\\n",
    "            budget, opening_weekend_gross, production_company, sound_mix, color, aspect_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_multiple_individual_movie_pages(links):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    links = list of links to individual IMDB movie pages to scrape\n",
    "    \n",
    "    Out:\n",
    "    detailed_movie_data = list of list of movie data, with each sub-list corresponding to a single movie. Each sublist\n",
    "                          contains release_date, critic_rating, critic_rating_n, writer, country, language, budget, \n",
    "                          opening_weekend_gross, production_company, sound_mix, color, aspect_ratio\n",
    "    \"\"\"\n",
    "    detailed_movie_data = []\n",
    "    links_visited = 0\n",
    "    \n",
    "    # Go through the list of links and scrape each individual page\n",
    "    for link in links:\n",
    "        movie_html = get_HTML(link)\n",
    "        data = [link]\n",
    "        data += scrape_individual_movie_page(movie_html)\n",
    "        detailed_movie_data.append(data)\n",
    "        links_visited += 1\n",
    "        \"\"\"\n",
    "        Note: scrape individual pages is quite time consuming. If you are trying to do a lot at once. It is recommend\n",
    "        to periodically save (pickle) your results so that you don't lose it all if something goes wrong in the code\n",
    "        on some random page, your signal is cut/interupted, etc.\n",
    "        \n",
    "        To do this uncomment the lines below and update to your preference. Currently saves progress every 1,000 pages\n",
    "        \n",
    "        \n",
    "        if links_visited % 1000 == 0:\n",
    "            pickle_it(detailed_movie_data, \"detailed_movie_data.pkl\")\n",
    "        \"\"\"\n",
    "     \n",
    "    \"\"\"Uncomment this to save (pickle) the scraped data once you've gone through all links (as another failsafe)\"\"\"\n",
    "    # pickle_it(detailed_movie_data, \"detailed_movie_data.pkl\")\n",
    "    \n",
    "    return detailed_movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load individual IMDB page links to scrape from pickle\n",
    "all_links = load_pickle(\"all_links.pkl\")\n",
    "\n",
    "\"\"\"Uncomment these if you want to scrape the data\"\"\"\n",
    "# detailed_movie_data = scrape_multiple_individual_movie_pages(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Uncomment this when you want to save the data you scraped\"\"\"\n",
    "# pickle_it(detailed_movie_data, \"detailed_movie_data.pkl\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
